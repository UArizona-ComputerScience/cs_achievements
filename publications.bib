@inproceedings{akoju-etal-2023-synthetic,
title = "Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference",
author = "Akoju, Sushma Anand and Vacareanu, Robert and Blanco, Eduardo and Riaz, Haris and Surdeanu, Mihai",
editor = "Dalvi Mishra, Bhavana and Durrett, Greg and Jansen, Peter and Neves Ribeiro, Danilo and Wei, Jason",
booktitle = "Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)",
month = jun,
year = "2023",
address = "Toronto, Canada",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2023.nlrse-1.12; https://arxiv.org/abs/2307.05034"
doi = "10.18653/v1/2023.nlrse-1.12; https://doi.org/10.48550/arXiv.2307.05034",
pages = "157--168",
}

@inproceedings{george-surdeanu-2023-sexually,
    title = "It{'}s not Sexually Suggestive; It{'}s Educative | Separating Sex Education from Suggestive Content on {T}ik{T}ok videos",
    author = "George, Enfa  and Surdeanu, Mihai",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.365",
    doi = "10.18653/v1/2023.findings-acl.365",
    pages = "5904--5915",
    abstract = "We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled as sexually suggestive (from the annotator{'}s point of view), sex-educational content, or neither. Such a dataset is necessary to address the challenge of distinguishing between sexually suggestive content and virtual sex education videos on TikTok. Children{'}s exposure to sexually suggestive videos has been shown to have adversarial effects on their development (Collins et al. 2017). Meanwhile, virtual sex education, especially on subjects that are more relevant to the LGBTQIA+ community, is very valuable (Mitchell et al. 2014). The platform{'}s current system removes/punishes some of both types of videos, even though they serve different purposes. Our dataset contains video URLs, and it is also audio transcribed. To validate its importance, we explore two transformer-based models for classifying the videos. Our preliminary results suggest that the task of distinguishing between these types of videos is learnable but challenging. These experiments suggest that this dataset is meaningful and invites further study on the subject.",
}

@inproceedings{findings_emlp2023_zijie,
  title = {Interpreting Indirect Answers to Yes-No Questions in Multiple Languages},
  author = {Wang, Zijie and Hossain, Md Mosharaf and Mathur, Shivam and Melo, Terry Cruz and Ozler, Kadir Bulut and Park, Keun Hee and Quintero, Jacob and Rezaei, MohammadHossein and Shakya, Shreya Nupur and Uddin, Md Nayem and Blanco, Eduardo},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  month = dec,
  year = {2023},
  address = {Singapore},
  publisher = {Association for Computational Linguistics},
  month_numeric = {12}
}

@inproceedings{zhao2023revisiting,
  title={Revisiting Simple Regret: Fast Rates for Returning a Good Arm},
  author={Zhao, Yao and Stephens, Connor and Szepesvari, Csaba and Jun, Kwang-Sung},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2023},
  organization={PMLR}
}

@inproceedings{rahimi2023improving,
  title={Improving Zero-shot Relation Classification via Automatically-acquired Entailment Templates},
  author={Rahimi, Mahdi and Surdeanu, Mihai},
  booktitle={Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)},
  pages={187--195},
  year={2023}
}

@inproceedings{suresh2023intermediate,
  title={Intermediate Domain Finetuning for Weakly Supervised Domain-adaptive Clinical NER},
  author={Suresh, Shilpa and Tavabi, Nazgol and Golchin, Shahriar and Gilreath, Leah and Garcia-Andujar, Rafael and Kim, Alexander and Murray, Joseph and Bacevich, Blake and Kiapour, Ata},
  booktitle={The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks},
  pages={320--325},
  year={2023}
}

@inproceedings{golchin2023not,
  title={Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords},
  author={Golchin, Shahriar and Surdeanu, Mihai and Tavabi, Nazgol and Kiapour, Ata},
  booktitle={Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)},
  pages={13--21},
  year={2023}
}

@article{tavabi2023disparities,
  title={Disparities in cannabis use and documentation in electronic health records among children and young adults},
  author={Tavabi, Nazgol and Raza, Marium and Singh, Mallika and Golchin, Shahriar and Singh, Harsev and Hogue, Grant D and Kiapour, Ata M},
  journal={NPJ digital medicine},
  volume={6},
  number={1},
  pages={138},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{hassan2023automated,
  title={Automated Supervised Topic Modeling Framework for Hardware Weaknesses},
  author={Hassan, Rakibul and Bandi, Charan and Tsai, Meng-Tien and Golchin, Shahriar and PD, Sai Manoj and Rafatirad, Setareh and Salehi, Soheil},
  booktitle={2023 24th International Symposium on Quality Electronic Design (ISQED)},
  pages={1--8},
  year={2023},
  organization={IEEE}
}

@InProceedings{Ali2023ESORICS,
title = {A Tale of Reduction, Security and Correctness: Evaluating Program Debloating Paradigms and Their Compositions},
author = {Muaz Ali and Muhammad Muzammil and Faraz Karim and Ayesha Naeem and Rukhshan Haroon and Muhammad Haris and Huzaifa Nadeem and Waseem Sabir and Fahad Shaon and Fareed Zaffar and Vinod Yegneswaran and Ashish Gehani and Sazzadur Rahaman	},
booktitle="Computer Security -- ESORICS 2023",
year="2023",
publisher="Springer International Publishing",
abstract="Automated software debloating of program source or binary code has tremendous potential to improve both application performance and security. Unfortunately, measuring and comparing the effectiveness of various debloating methods is challenging due to the absence of a universal benchmarking platform that can accommodate diverse approaches. In this paper, first, we present ProdeBench, an extensible and sustainable benchmarking platform that enables comparison of different research techniques. Then, we perform a holistic comparison of the techniques and explore the various hidden and explicit tradeoffs in using them.",
}

@INPROCEEDINGS{Ali2023SecDev,
  author={Ali, Muaz and Habib, Rumaisa and Gehani, Ashish and Rahaman, Sazzadur and Uzmi, Zartash},
  booktitle={2023 IEEE Secure Development Conference (SecDev)}, 
  title={Blade: Scalable Source Code Debloating Framework}, 
  year={2023},
}


@inproceedings{NoriegaAtala2022NeuralGuidedPS,
    title = {Neural-Guided Program Synthesis of Information Extraction Rules Using Self-Supervision},
    author = {Enrique Noriega-Atala and Robert Vacareanu and Gus Hahn-Powell and Marco Antonio Valenzuela-Escarcega},
    booktitle = {PANDL},
    abstract="{We propose a neural-based approach for rule synthesis designed to help bridge the gap between the interpretability, precision and maintainability exhibited by rule-based information extraction systems with the scalability and convenience of statistical information extraction systems. This is achieved by avoiding placing the burden of learning another specialized language on domain experts and instead asking them to provide a small set of examples in the form of highlighted spans of text. We introduce a transformer-based architecture that drives a rule synthesis system that leverages a self-supervised approach for pre-training a large-scale language model complemented by an analysis of different loss functions and aggregation mechanisms for variable length sequences of user-annotated spans of text. The results are encouraging and point to different desirable properties, such as speed and quality, depending on the choice of loss and aggregation method. }",
    url = {https://aclanthology.org/2022.pandl-1.10.pdf},
    year = {2022}
}

@inproceedings{Vacareanu2022PatternRankJR,
    title = {PatternRank: Jointly Ranking Patterns and Extractions for Relation Extraction Using Graph-Based Algorithms},
    author = {Robert Vacareanu and Dane Bell and Mihai Surdeanu},
    booktitle = {PANDL},
    abstract="{In this paper we revisit the direction of using lexico-syntactic patterns for relation extraction instead of today's ubiquitous neural classifiers. We propose a semi-supervised graph-based algorithm for pattern acquisition that scores patterns and the relations they extract jointly, using a variant of PageRank. We insert light supervision in the form of seed patterns or relations, and model it with several custom teleportation probabilities that bias random-walk scores of patterns/relations based on their proximity to correct information. We evaluate our approach on Few-Shot TACRED, and show that our method outperforms (or performs competitively with) more expensive and opaque deep neural networks. Lastly, we thoroughly compare our proposed approach with the seminal RlogF pattern acquisition algorithm of, showing that it outperforms it for all the hyper parameters tested, in all settings. }",
    url = {https://aclanthology.org/2022.pandl-1.1.pdf},
    year = {2022}
}

@article{Varia2022InstructionTF,
    title = {Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis},
    author = {Siddharth Varia and Shuai Wang and Kishaloy Halder and Robert Vacareanu and Miguel Ballesteros and Yassine Benajiba and Neha Ann John and Rishita Anubhai and Smaranda Muresan and Dan Roth},
    journal = {ArXiv},
    abstract="{Aspect-based Sentiment Analysis (ABSA) is a ﬁne-grained sentiment analysis task which involves four elements from user-generated texts: aspect term, aspect category, opinion term, and sentiment polarity. Most computational approaches focus on some of the ABSA sub-tasks such as tuple (aspect term, sentiment polarity) or triplet (aspect term, opinion term, sentiment polarity) extraction using either pipeline or joint modeling approaches. Recently, generative approaches have been proposed to extract all four elements as (one or more) quadruplets from text as a single task. In this work, we take a step further and propose a uniﬁed framework for solving ABSA, and the associated sub-tasks to improve the performance in few-shot scenarios. To this end, we ﬁne-tune a T5 model with instructional prompts in a multi-task learning fashion covering all the sub-tasks, as well as the entire quadruple prediction task. In experiments with multiple benchmark data sets, we show that the proposed multi-task prompting approach brings performance boost (by abso-lute 6 . 75 F1) in the few-shot learning setting.}",
    year = {2022},
    url = {https://arxiv.org/pdf/2210.06629.pdf},
    volume = {abs/2210.06629}
}

@article{Vacareanu2022AHI,
    title = {A Human-machine Interface for Few-shot Rule Synthesis for Information Extraction},
    author = {Robert Vacareanu and George Caique Gouveia Barbosa and Enrique Noriega-Atala and Gus Hahn-Powell and Rebecca Sharp and Marco Antonio Valenzuela-Escarcega and Mihai Surdeanu},
    journal = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations},
    abstract = "{We propose a system that assists a user in constructing transparent information extraction models, consisting of patterns (or rules) written in a declarative language, through program synthesis.Users of our system can specify their requirements through the use of examples,which are collected with a search interface.The rule-synthesis system proposes rule candidates and the results of applying them on a textual corpus; the user has the option to accept the candidate, request another option, or adjust the examples provided to the system.Through an interactive evaluation, we show that our approach generates high-precision rules even in a 1-shot setting. On a second evaluation on a widely-used relation extraction dataset (TACRED), our method generates rules that outperform considerably manually written patterns.Our code, demo, and documentation is available at https://clulab.github.io/odinsynth.}",
    url = {https://aclanthology.org/2022.naacl-demo.8.pdf},
    year = {2022}
}

@article{Vacareanu2022FromET,
    title = {From Examples to Rules: Neural Guided Rule Synthesis for Information Extraction},
    author = {Robert Vacareanu and Marco Antonio Valenzuela-Escarcega and George Caique Gouveia Barbosa and Rebecca Sharp and Mihai Surdeanu},
    booktitle = {International Conference on Language Resources and Evaluation},
    abstract = "{While deep learning approaches to information extraction have had many successes, they can be difficult to augment or maintain as needs shift. Rule-based methods, on the other hand, can be more easily modified. However, crafting rules requires expertise in linguistics and the domain of interest, making it infeasible for most users. Here we attempt to combine the advantages of these two directions while mitigating their drawbacks. We adapt recent advances from the adjacent field of program synthesis to information extraction, synthesizing rules from provided examples. We use a transformer-based architecture to guide an enumerative search, and show that this reduces the number of steps that need to be explored before a rule is found. Further, we show that without training the synthesis algorithm on the specific domain, our synthesized rules achieve state-of-the-art performance on the 1-shot scenario of a task that focuses on few-shot learning for relation classification, and competitive performance in the 5-shot scenario.}",
    url = {https://aclanthology.org/2022.lrec-1.665.pdf},
    year = {2022},
}

@inproceedings{Vacareanu2020AnUM,
    title = {An Unsupervised Method for Learning Representations of Multi-word Expressions for Semantic Classification},
    author = {Robert Vacareanu and Marco Antonio Valenzuela-Escarcega and Rebecca Sharp and Mihai Surdeanu},
    booktitle = {International Conference on Computational Linguistics},
    abstract = "{This paper explores an unsupervised approach to learning a compositional representation function for multi-word expressions (MWEs), and evaluates it on the Tratz dataset, which associates two-word expressions with the semantic relation between the compound constituents (e.g. the label employer is associated with the noun compound government agency) (Tratz, 2011). The composition function is based on recurrent neural networks, and is trained using the Skip-Gram objective to predict the words in the context of MWEs. Thus our approach can naturally leverage large unlabeled text sources. Further, our method can make use of provided MWEs when available, but can also function as a completely unsupervised algorithm, using MWE boundaries predicted by a single, domain-agnostic part-of-speech pattern. With pre-defined MWE boundaries, our method outperforms the previous state-of-the-art performance on the coarse-grained evaluation of the Tratz dataset (Tratz, 2011), with an F1 score of 50.4%. The unsupervised version of our method approaches the performance of the supervised one, and even outperforms it in some configurations. }",
    url = {https://www.aclweb.org/anthology/2020.coling-main.297.pdf},
    year = {2020}
}

@inproceedings{Vacareanu2020ParsingAT,
    title = {Parsing as Tagging},
    author = {Robert Vacareanu and George Caique Gouveia Barbosa and Marco Antonio Valenzuela-Escarcega and Mihai Surdeanu},
    booktitle = {International Conference on Language Resources and Evaluation},
    abstract = "{We propose a simple yet accurate method for dependency parsing that treats parsing as tagging (PaT). That is, our approach addresses the parsing of dependency trees with a sequence model implemented with a bidirectional LSTM over BERT embeddings, where the “tag” to be predicted at each token position is the relative position of the corresponding head. For example, for the sentence John eats cake, the tag to be predicted for the token cake is -1 because its head (eats) occurs one token to the left. Despite its simplicity, our approach performs well. For example, our approach outperforms the state-of-the-art method of (Fernández-González and Gómez-Rodríguez, 2019) on Universal Dependencies (UD) by 1.76% unlabeled attachment score (UAS) for English, 1.98% UAS for French, and 1.16% UAS for German. On average, on 12 UD languages, our method with minimal tuning performs comparably with this state-of-the-art approach: better by 0.11% UAS, and worse by 0.58% LAS.}",
    url = {https://www.aclweb.org/anthology/2020.lrec-1.643.pdf},
    year = {2020}
}


@article{10.1162/coli_a_00463,
    author = {Tang, Zheng and Surdeanu, Mihai},
    title = "{It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers}",
    journal = {Computational Linguistics},
    pages = {1-40},
    year = {2022},
    month = {09},
    abstract = "{We propose an explainable approach for relation extraction that mitigates the tension between generalization and explainability by jointly training for the two goals. Our approach uses a multi-task learning architecture, which jointly trains a classifier for relation extraction, and a sequence model that labels words in the context of the relation that explain the decisions of the relation classifier. We also convert the model outputs to rules to bring global explanations to this approach. This sequence model is trained using a hybrid strategy: supervised, when supervision from pre-existing patterns is available, and semi-supervised otherwise. In the latter situation, we treat the sequence model’s labels as latent variables, and learn the best assignment that maximizes the performance of the relation classifier. We evaluate the proposed approach on the two datasets and show that the sequence model provides labels that serve as accurate explanations for the relation classifier’s decisions, and, importantly, that the joint training generally improves the performance of the relation classifier. We also evaluate the performance of the generated rules and show that the new rules are great add-on to the manual rules and bring the rule-based system much closer to the neural models.}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00463},
    url = {https://doi.org/10.1162/coli\_a\_00463},
    eprint = {https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli\_a\_00463/2046371/coli\_a\_00463.pdf},
}


@article{zahedi2022prediction,
  title={Prediction of blast loading on protruded structures using machine learning methods},
  author={Zahedi, Mona and Golchin, Shahriar},
  journal={International Journal of Protective Structures},
  pages={20414196221144067},
  year={2022},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{eduong2023tomcat,
  title={The ToMCAT Dataset},
  author={Pyarelal, Adarsh and Duong, Eric and Shibu, Caleb and Soares, Paulo and Boyd, Savannah and Khosla, Payal and Pfeifer, Valeria A. and Zhang, Diheng and Andrews, Eric and Champlin, Rick and Raymond, Vincent and Krishnaswamy, Meghavarshini and Morrison, Clayton and Butler, Emily and Barnard, Kobus},
  journal={NeurIPS 2023},
  year={2023},
  publisher={NeurIPS}
}

@article{eshghali2023machine,
  title={Machine learning based integrated scheduling and rescheduling for elective and emergency patients in the operating theatre},
  author={Eshghali, Masoud and Kannan, Devika and Salmanzadeh-Meydani, Navid and Esmaieeli Sikaroudi, Amir Mohammad},
  journal={Annals of Operations Research},
  pages={1--24},
  year={2023},
  publisher={Springer}
}
@inproceedings{10.1145/3578360.3580260,
  author = {Lim, HeuiChan and Debray, Saumya},
  title = {Automatically Localizing Dynamic Code Generation Bugs in JIT Compiler Back-End},
  year = {2023},
  isbn = {9798400700880},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3578360.3580260},
  doi = {10.1145/3578360.3580260},
  abstract = {Just-in-Time (JIT) compilers are ubiquitous in modern computing systems and are used in a wide variety of software. Dynamic code generation bugs, where the JIT compiler silently emits incorrect code, can result in exploitable vulnerabilities. They, therefore, pose serious security concerns and make quick mitigation essential. However, due to the size and complexity of JIT compilers, quickly locating and fixing bugs is often challenging. In addition, the unique characteristics of JIT compilers make existing bug localization approaches inapplicable. Therefore, this paper proposes a new approach to automatic bug localization, explicitly targeting the JIT compiler back-end. The approach is based on explicitly modeling architecture-independent back-end representation and architecture-specific code-generation. Experiments using a prototype implementation on a widely used JIT compiler (Turbofan) indicate that it can successfully localize dynamic code generation bugs in the back-end with high accuracy.},
  booktitle = {Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction},
  pages = {145–155},
  numpages = {11},
  keywords = {JIT Compiler, Dynamic Program Analysis, Back-End, Automatic Bug Localization, Dynamic Code Generation},
  location = {Montr\'{e}al, QC, Canada},
  series = {CC 2023}
}
